Here is the full technical specification for Tasks 4 and 5, detailing the implementation of a Rust-native topic representation module and the subsequent pruned graph construction.

This document assumes all prerequisite tasks (1. Load Data, 2. UMAP Reduction, 3. Clustering) have been completed.

-----

### **Technical Specification: Rust-Native Topic Graph Generation Pipeline (Tasks 4 & 5)**

#### **1. Overview**

This specification details a high-performance, pure-Rust pipeline to analyze a collection of documents and their associated LLM-generated embeddings. The pipeline will consume the output of a clustering algorithm (Task 3) to first generate meaningful keyword-based representations for each topic (Task 4) and then construct a pruned, serializable graph (Task 5) suitable for UI visualization.

#### **2. Prerequisites (Inputs for Task 4)**

This pipeline must be provided with the following data, which is assumed to be in memory:

1.  **`all_models: Vec<document::Model>`**: A `Vec` of `sea-orm` models (or equivalent structs) loaded from the database. Each model must contain:
      * `id: i64` (or `Uuid`): A unique document identifier.
      * `document_text: String`: The raw text content.
      * `embedding: Vec<f32>`: The original, high-dimensional LLM embedding.
2.  **`cluster_assignments: Vec<Option<usize>>`**: The output from the `linfa-clustering` (Task 3). This list maps each document in `all_models` (by index) to a topic ID (e.g., `Some(0)`, `Some(1)`) or an outlier group (`None`).

-----

### **Task 4: c-TF-IDF Topic Representation**

#### **4.1. Objective**

To assign a set of meaningful, descriptive keywords to each topic ID generated by the clustering step. This is achieved by implementing the **Class-based Term Frequency-Inverse Document Frequency (c-TF-IDF)** algorithm.

#### **4.2. Core Concept**

The c-TF-IDF algorithm finds the most representative words for a topic by treating all documents belonging to that topic as a *single, large document*. It then calculates TF-IDF scores for words based on these "topic documents."

  * **Class-based TF (Term Frequency):** The frequency of a word in a specific topic, normalized by the total number of words in that topic.
  * **Class-based IDF (Inverse Document Frequency):** The rarity of a word across all *topics*, not all documents.

#### **4.3. Required Crates**

  * `ahash = "0.8"`: For high-performance `AHashMap` and `AHashSet`.
  * `stopwords = "0.2"`: For loading standard English stop-word lists.
  * `rayon = "1.10"`: For high-performance parallel processing.
  * `regex = "1.10"`: For efficient, word-boundary-aware tokenization.

#### **4.4. Data Structures**

  * **`TopicTfData` (Intermediate):**
    ```rust
    struct TopicTfData {
        word_counts: AHashMap<String, u64>,
        total_words: u64,
        doc_count: usize,
    }
    ```
  * **`TopicInfo` (Output):**
    ```rust
    pub struct TopicInfo {
        pub top_words: Vec<(String, f64)>, // (Word, c-TF-IDF Score)
        pub doc_count: usize,
    }
    ```

#### **4.5. Sub-steps**

**4.5.1. Parallel Tokenization & Stop Word Removal**

  * **Action:** Convert all raw document texts into lists of clean tokens.
  * **Implementation:**
    1.  Load the English stop-word set into an `AHashSet<String>`: `stopwords::get_sync(LANGUAGE::English)`.
    2.  Define a tokenization `Regex`: `let re = Regex::new(r"\b\w+\b").unwrap();`.
    3.  Use `rayon` to process the `all_document_texts` `Vec<String>` in parallel (`.par_iter()`).
    4.  For each document, apply a mapping function that:
          * Converts text to lowercase.
          * Uses `re.find_iter` to extract words.
          * Filters out any word present in the `AHashSet` of stop words or with a length `<= 2`.
          * Collects the results into a `Vec<String>`.
    5.  Collect the final parallel results into `tokenized_docs: Vec<Vec<String>>`.

**4.5.2. Group Tokenized Documents by Topic**

  * **Action:** Assign each list of tokens to its corresponding topic bucket.
  * **Implementation:**
    1.  Initialize `grouped_docs: AHashMap<usize, Vec<Vec<String>>>`.
    2.  Iterate from `i in 0..tokenized_docs.len()`.
    3.  `if let Some(topic_id) = cluster_assignments[i]` (from Task 3), push `tokenized_docs[i]` into the `grouped_docs.entry(topic_id).or_default()` vector.
    4.  Documents with `None` (outliers) are implicitly ignored.

**4.5.3. Calculate Term Frequency (TF) per Topic**

  * **Action:** Aggregate word counts for each topic to create the "topic documents."
  * **Implementation:**
    1.  Use `grouped_docs.par_iter()` to process all topic buckets in parallel.
    2.  For each `(topic_id, docs_in_topic)`:
          * Initialize `word_counts: AHashMap<String, u64>` and `total_words: u64 = 0`.
          * Iterate through every `doc` in `docs_in_topic`, and every `word` in `doc`.
          * Increment the count: `*word_counts.entry(word.clone()).or_insert(0) += 1;`.
          * Increment `total_words += 1;`.
    3.  Collect the results into `tf_data: AHashMap<usize, TopicTfData>`, storing `word_counts`, `total_words`, and `doc_count: docs_in_topic.len()`.

**4.5.4. Calculate Inverse Document Frequency (IDF)**

  * **Action:** Calculate the global rarity score for each word across all *topics*.
  * **Implementation:**
    1.  `let total_topics = tf_data.len() as f64;`.
    2.  Initialize `topic_freqs: AHashMap<String, u64>`.
    3.  Iterate over `tf_data.values()` and for each `word` in `topic.word_counts.keys()`, increment `*topic_freqs.entry(word.clone()).or_insert(0) += 1;`.
    4.  Initialize `idf_scores: AHashMap<String, f64>`.
    5.  Iterate over `topic_freqs.iter()`.
    6.  **c-IDF Formula:** `let idf_score = (total_topics / (count as f64 + 1.0)).ln() + 1.0;`
          * `+ 1.0` (denominator): Laplace smoothing.
          * `+ 1.0` (end): "Smooth IDF" to ensure non-negative scores.
    7.  Store the `idf_score` in the `idf_scores` map.

**4.5.5. Calculate Final c-TF-IDF & Extract Top Words**

  * **Action:** Combine TF and IDF scores and find the top N words for each topic.
  * **Implementation:**
    1.  Use `tf_data.par_iter()` to process all topics in parallel.
    2.  For each `(topic_id, topic_data)`:
          * Initialize `scores: Vec<(String, f64)>`.
          * `let total_words_in_topic = topic_data.total_words as f64;`.
          * Iterate `(word, tf_count)` in `topic_data.word_counts`.
          * **TF:** `let tf_score = (*tf_count as f64) / total_words_in_topic;`
          * **IDF:** `let idf_score = idf_scores.get(word).cloned().unwrap_or(0.0);`
          * **Final Score:** `let c_tfidf_score = tf_score * idf_score;`
          * Push `(word.clone(), c_tfidf_score)` to `scores`.
    3.  Sort `scores` by score (descending): `scores.sort_unstable_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));`.
    4.  Take the top 10: `let top_words = scores.into_iter().take(10).collect();`.
    5.  Collect the results into the final output map.

#### **4.6. Task 4 Output**

  * **`final_topic_info: AHashMap<usize, TopicInfo>`**: A map where the key is the `topic_id` and the value is a `TopicInfo` struct containing the topic's top 10 keywords and its total document count.

-----

### **Task 5: Graph Construction & Pruning**

#### **5.1. Objective**

To convert the topic data (from Task 4) and document data (from Prerequisites) into a serializable graph data structure. This process will be governed by a **pruning configuration** to ensure the default graph is simple, clean, and performant for UI rendering.

#### **5.2. Required Crates**

  * `serde = { version = "1.0", features = ["derive"] }`
  * `serde_json = "1.0"`
  * `ndarray = "0.15"`
  * `ndarray-linalg = "0.16"` (For cosine similarity, or implement manually)
  * `ahash = "0.8"`

#### **5.3. Data Structures**

  * **Configuration:**
    ```rust
    #[derive(PartialEq, Clone, Copy)]
    pub enum GraphDetailLevel {
        TopicsOnly, // Default: Only show Topic nodes and their connections
        FullGraph,  // Show all document nodes
    }

    pub struct GraphPruningConfig {
        pub detail_level: GraphDetailLevel,
        pub min_topic_size: usize,       // Pruning: Ignore topics with fewer docs
        pub topic_edge_threshold: f64, // Pruning: Ignore weak topic-topic links
    }
    ```
  * **Serialization (Output):**
    ```rust
    use serde::Serialize;

    #[derive(Serialize)]
    struct GraphNode {
        id: String,          // e.g., "topic_0" or "doc_123"
        label: String,       // e.g., "finance, market,..." or "Q3 Report..."
        #[serde(rename = "type")]
        node_type: String, // "topic" or "document"
        size: f64,           // e.g., document count for topics
    }

    #[derive(Serialize)]
    struct GraphEdge {
        id: String,
        from: String,
        to: String,
        #[serde(skip_serializing_if = "Option::is_none")]
        weight: Option<f64>, // For topic-topic similarity
    }

    #[derive(Serialize)]
    pub struct GraphData {
        nodes: Vec<GraphNode>,
        edges: Vec<GraphEdge>,
    }
    ```

#### **5.4. Sub-steps**

**5.4.1. Calculate Topic Vectors & Similarity**

  * **Action:** Calculate the average embedding (centroid) for each topic. This requires the *original, high-dimensional* embeddings.
  * **Implementation:**
    1.  Initialize `topic_vector_sum: AHashMap<usize, (Array1<f32>, usize)>`. (Stores sum and count).
    2.  Iterate from `i in 0..all_models.len()`.
    3.  `if let Some(topic_id) = cluster_assignments[i]`:
          * Convert `all_models[i].embedding` to `Array1<f32>`.
          * `let (sum, count) = topic_vector_sum.entry(topic_id).or_default();`
          * `*sum += &embedding;`
          * `*count += 1;`
    4.  Create `avg_topic_vectors: AHashMap<usize, Array1<f32>>` by iterating `topic_vector_sum` and dividing each `sum` by its `count`.
    5.  Create `topic_similarity: AHashMap<(usize, usize), f64>` by computing the cosine similarity for every pair of vectors in `avg_topic_vectors`.

**5.4.2. Filter Dominant Topics**

  * **Action:** Create an "allow-list" of topics based on the pruning configuration.
  * **Implementation:**
    1.  `let config: GraphPruningConfig = ...;` (Initialize with defaults, e.g., `TopicsOnly`, `min_topic_size: 10`).
    2.  Create `dominant_topics: AHashSet<usize>` by filtering `final_topic_info` (from Task 4.6) where `info.doc_count >= config.min_topic_size`.

**5.4.3. Construct Final Graph**

  * **Action:** Build the `Vec<GraphNode>` and `Vec<GraphEdge>` using the pruning filters.
  * **Implementation:**
    1.  Initialize `let mut nodes = Vec::new();` and `let mut edges = Vec::new();`.
    2.  **Create Topic Nodes:**
          * Iterate over `final_topic_info.iter()`.
          * `if !dominant_topics.contains(topic_id) { continue; }`.
          * Create the node `label` from `info.top_words`.
          * `nodes.push(GraphNode { id: format!("topic_{}", topic_id), label, ... });`.
    3.  **Create Document Nodes & Doc-Topic Edges (Conditional):**
          * `if config.detail_level == GraphDetailLevel::FullGraph { ... }`.
          * Inside this block, iterate from `i in 0..all_models.len()`.
          * `if let Some(topic_id) = cluster_assignments[i]`.
          * Check `if dominant_topics.contains(&topic_id)` (This prunes docs belonging to small topics).
          * If true:
              * `let doc_model = &all_models[i];`
              * `let doc_id_str = format!("doc_{}", doc_model.id);`
              * `let doc_label = doc_model.document_text.chars().take(50).collect();`
              * `nodes.push(GraphNode { id: doc_id_str.clone(), label: doc_label, ... });`
              * `edges.push(GraphEdge { id: format!("edge_{}_to_topic_{}", doc_id_str, topic_id), from: doc_id_str, to: format!("topic_{}", topic_id), ... });`
    4.  **Create Topic-Topic Edges:**
          * Iterate over `topic_similarity` (from Step 5.4.1).
          * `if score > config.topic_edge_threshold && dominant_topics.contains(&topic_a) && dominant_topics.contains(&topic_b)`:
          * `edges.push(GraphEdge { id: format!("edge_topic_{}_to_{}", topic_a, topic_b), from: format!("topic_{}", topic_a), to: format!("topic_{}", topic_b), weight: Some(score) });`

**5.4.4. Serialize Final Output**

  * **Action:** Convert the graph data into a JSON string.
  * **Implementation:**
    1.  `let final_graph = GraphData { nodes, edges };`
    2.  `let json_output: String = serde_json::to_string(&final_graph)?;`

#### **5.5. Task 5 Output (Final Pipeline Result)**

A single JSON string representing the pruned graph, ready to be sent to a UI.

**Example (for `GraphDetailLevel::TopicsOnly`):**

```json
{
  "nodes": [
    {
      "id": "topic_0",
      "label": "finance, market, stock, revenue, quarter, shares",
      "type": "topic",
      "size": 52.0
    },
    {
      "id": "topic_1",
      "label": "model, training, layer, ai, network, parameters",
      "type": "topic",
      "size": 38.0
    }
  ],
  "edges": [
    {
      "id": "edge_topic_0_to_1",
      "from": "topic_0",
      "to": "topic_1",
      "weight": 0.72
    }
  ]
}
```
