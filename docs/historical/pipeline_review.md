# DAG Pipeline Scalability Review

## Key Findings
- **Datasource ingestion bottlenecks** (`layercake-core/src/pipeline/dataset_importer.rs:57`): CSV/TSV files are fully loaded into memory via `std::fs::read_to_string`, then each record is inserted into `dataset_rows` with individual `insert` calls. Large uploads will double memory usage (reader + serde maps) and execute O(n) round-trips without batching or transactions.
- **JSON graph handling** (`layercake-core/src/pipeline/dataset_importer.rs:173`): Entire graphs are parsed into a single `serde_json::Value` and stored as one row. This prevents streaming, duplicates the payload multiple times in memory, and risks exceeding row-size limits as graphs grow.
- **Graph build path re-materializes everything** (`layercake-core/src/pipeline/graph_builder.rs:150`, `graph_builder.rs:522`): For each upstream node the builder fetches all rows, parses JSON, accumulates nodes/edges/layers in `HashMap`/`Vec`, and then issues per-record inserts after truncating the destination tables. Large graphs are reloaded and rewritten in full even when only a slice changes.
- **Merge builder repeats the same pattern** (`layercake-core/src/pipeline/merge_builder.rs:94`, `merge_builder.rs:204`): Upstream graphs are fetched node-by-node, converted back into JSON-like maps, stored in aggregate collections, and persisted row-by-row. Every merge step duplicates data already present in the database and multiplies memory pressure for branched DAGs.
- **Transform execution round-trips full graphs** (`layercake-core/src/pipeline/dag_executor.rs:134`, `graph_service.rs:22`): `execute_transform_node` asks `GraphService` to rebuild in-memory `Graph` structs from the database, applies transforms, truncates the destination graph tables, and reinserts every node/edge. Multi-stage pipelines repeatedly deserialize and rewrite identical graphs.
- **Sequential, single-threaded DAG walks** (`layercake-core/src/pipeline/dag_executor.rs:233`): Execution is strictly ordered even for independent branches. Each node waits for its predecessors to finish, so wide DAGs cannot exploit available cores or separate database connections.
- **Hashing ignores payload changes** (`layercake-core/src/pipeline/graph_builder.rs:372`, `merge_builder.rs:188`): Change detection only looks at IDs, filenames, and timestamps. If a large dataset is mutated without bumping `processed_at`, downstream nodes still rebuild everything, while small updates with touching timestamps still force a full truncate-and-rewrite.

## Recommendations
- **Stream and batch dataset ingestion**: Use `csv::Reader::from_path` or buffered streams plus chunked SeaORM `insert_many`/`bulk_insert` inside explicit transactions. For very large imports consider leveraging the database's native COPY interface or writing directly to staging tables with `sea_orm::Statement`. This removes the doubled memory footprint and collapses millions of inserts into single round-trips.
- **Normalize graph persistence layers**: Instead of storing large JSON blobs, persist node/edge records directly as they stream in, or keep them in parquet/arrow files referenced by path. If JSON needs to stay, compress or shard it to avoid overgrown rows.
- **Make graph builds incremental**: Replace the current truncate-and-reload cycle with diff-aware updates. Track hashes per dataset slice (e.g., per layer or partition) and upsert only the changed nodes/edges. Alternatively, stage rebuilt graphs in temporary tables and swap them with `INSERT ... SELECT`/`DELETE` statements to keep work inside the database.
- **Reuse existing graph state**: For transform nodes, fetch only the delta or apply transforms with `UPDATE`/`DELETE` SQL where possible. When a full materialization is required, insert via batched statements and avoid re-serializing attributes already stored in JSON.
- **Introduce execution parallelism & backpressure**: Execute independent branches concurrently using a bounded task pool keyed by in-degree counts. Combine this with per-node async tasks that reuse one transaction per node to reduce scheduling overhead.
- **Strengthen change detection**: Compute hashes over actual dataset contents (streamed checksums) and persist them alongside row counts. This allows skipping downstream work when the dataset truly has not changed and avoids unnecessary rewrites triggered by timestamp churn.
- **Add observability around pipeline stages**: Capture ingestion throughput, per-node execution times, number of rows/edges processed, and DB round-trip counts. This makes it easier to verify the impact of the above optimizations on real workloads.
