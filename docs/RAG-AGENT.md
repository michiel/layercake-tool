# RAG-Enabled Chat Agent Implementation Plan

## Overview

Enable chat agents to access the project's knowledge base using Retrieval Augmented Generation (RAG). This allows agents to answer questions based on uploaded documents while maintaining conversation context.

## Current Architecture

### Existing Components

1. **Vector Store** (`layercake-data-acquisition/src/vector_store.rs`)
   - `SqliteVectorStore` with cosine similarity search
   - Stores document chunks with embeddings
   - Indexed by project_id and file_id

2. **Embedding Service** (`layercake-data-acquisition/src/embeddings.rs`)
   - Supports OpenAI and Ollama providers
   - Generates embeddings for text chunks
   - Used during document ingestion

3. **Chat System** (`layercake-core/src/console/chat/session.rs`)
   - Manages conversation sessions
   - Uses `rig` crate for agent creation
   - Currently no RAG integration

4. **Data Acquisition Service** (`layercake-data-acquisition/src/services/mod.rs`)
   - `search_context()` method already exists
   - Returns `VectorSearchResult` with chunks and similarity scores

## Implementation Plan

### Phase 1: Core RAG Infrastructure

#### 1.1 Extend ChatSession with RAG Context

**File**: `layercake-core/src/console/chat/session.rs`

Add RAG configuration to `ChatSession`:

```rust
pub struct ChatSession {
    // ... existing fields ...

    /// Number of document chunks to retrieve for context (default: 5)
    pub rag_top_k: usize,

    /// Minimum similarity score for chunk inclusion (0.0-1.0, default: 0.7)
    pub rag_threshold: f32,

    /// Whether to include source citations in responses
    pub include_citations: bool,
}
```

**Rationale**: Configuration allows per-session tuning of RAG behavior.

#### 1.2 Create RAG Context Builder

**New File**: `layercake-core/src/console/chat/rag.rs`

```rust
use anyhow::Result;
use layercake_data_acquisition::vector_store::VectorSearchResult;

pub struct RagContext {
    pub chunks: Vec<RagChunk>,
    pub total_tokens: usize,
}

pub struct RagChunk {
    pub content: String,
    pub source: String,
    pub score: f32,
    pub file_id: String,
}

impl RagContext {
    /// Build context string for LLM prompt
    pub fn to_context_string(&self) -> String {
        let mut context = String::new();
        context.push_str("# Knowledge Base Context\n\n");

        for (i, chunk) in self.chunks.iter().enumerate() {
            context.push_str(&format!(
                "## Document {} (relevance: {:.2})\n",
                i + 1,
                chunk.score
            ));
            context.push_str(&format!("Source: {}\n\n", chunk.source));
            context.push_str(&chunk.content);
            context.push_str("\n\n---\n\n");
        }

        context
    }

    /// Get citations for response footer
    pub fn get_citations(&self) -> Vec<String> {
        self.chunks
            .iter()
            .enumerate()
            .map(|(i, chunk)| format!("[{}] {}", i + 1, chunk.source))
            .collect()
    }
}

pub struct RagContextBuilder {
    results: Vec<VectorSearchResult>,
    threshold: f32,
    max_tokens: usize,
}

impl RagContextBuilder {
    pub fn new(threshold: f32, max_tokens: usize) -> Self {
        Self {
            results: Vec::new(),
            threshold,
            max_tokens,
        }
    }

    pub fn add_results(mut self, results: Vec<VectorSearchResult>) -> Self {
        self.results = results;
        self
    }

    pub fn build(self) -> RagContext {
        let mut chunks = Vec::new();
        let mut total_tokens = 0;

        for result in self.results {
            // Filter by threshold
            if result.score < self.threshold {
                continue;
            }

            // Estimate tokens (rough: 1 token â‰ˆ 4 chars)
            let estimated_tokens = result.content.len() / 4;

            if total_tokens + estimated_tokens > self.max_tokens {
                break;
            }

            chunks.push(RagChunk {
                content: result.content.clone(),
                source: result.filename.unwrap_or_else(|| "Unknown".to_string()),
                score: result.score,
                file_id: result.file_id.to_string(),
            });

            total_tokens += estimated_tokens;
        }

        RagContext {
            chunks,
            total_tokens,
        }
    }
}
```

**Rationale**:
- Separates RAG logic from chat session logic
- Handles token budget management
- Provides formatted context and citations

#### 1.3 Integrate RAG into Chat Flow

**File**: `layercake-core/src/console/chat/session.rs`

Modify `chat()` method:

```rust
pub async fn chat(&mut self, user_message: &str) -> Result<String> {
    // 1. Get embedding for user query
    let query_embedding = self.app
        .data_acquisition_service()
        .embeddings
        .as_ref()
        .ok_or_else(|| anyhow!("Embeddings not configured"))?
        .embed_text(user_message)
        .await?;

    // 2. Search knowledge base
    let search_results = self.app
        .data_acquisition_service()
        .search_context(self.project_id, &query_embedding, self.rag_top_k)
        .await?;

    // 3. Build RAG context
    let rag_context = RagContextBuilder::new(self.rag_threshold, 4000)
        .add_results(search_results)
        .build();

    // 4. Build enhanced system prompt
    let mut enhanced_preamble = self.system_prompt.clone();
    if !rag_context.chunks.is_empty() {
        enhanced_preamble.push_str("\n\n");
        enhanced_preamble.push_str(&rag_context.to_context_string());
        enhanced_preamble.push_str("\n\nUse the above context to answer questions when relevant. ");
        enhanced_preamble.push_str("If the context doesn't contain relevant information, ");
        enhanced_preamble.push_str("say so and use your general knowledge.\n");
    }

    // 5. Create agent with enhanced preamble
    let agent = self.client
        .agent(&self.model)
        .preamble(&enhanced_preamble)
        .build();

    // 6. Get response
    let response = agent.prompt(user_message).await?;

    // 7. Add citations if enabled
    let mut final_response = response.clone();
    if self.include_citations && !rag_context.chunks.is_empty() {
        final_response.push_str("\n\n---\n**Sources:**\n");
        for citation in rag_context.get_citations() {
            final_response.push_str(&format!("- {}\n", citation));
        }
    }

    // 8. Save to history
    self.save_message("user", user_message)?;
    self.save_message("assistant", &final_response)?;

    Ok(final_response)
}
```

**Rationale**:
- Query embedding uses same model as document embeddings
- Context is prepended to system prompt (more reliable than appending to user message)
- Citations provide transparency
- Graceful degradation if no relevant context found

### Phase 2: GraphQL API Integration

#### 2.1 Add RAG Configuration to Chat Mutations

**File**: `layercake-core/src/graphql/types/chat.rs`

```rust
#[derive(InputObject)]
pub struct CreateChatSessionInput {
    pub project_id: i32,
    pub title: Option<String>,
    pub system_prompt: Option<String>,
    pub model: Option<String>,

    // RAG configuration
    pub enable_rag: Option<bool>,
    pub rag_top_k: Option<i32>,
    pub rag_threshold: Option<f64>,
    pub include_citations: Option<bool>,
}

#[derive(SimpleObject)]
pub struct ChatSession {
    pub id: i32,
    pub project_id: i32,
    pub title: String,
    pub system_prompt: String,
    pub model: String,
    pub created_at: DateTime<Utc>,

    // RAG fields
    pub enable_rag: bool,
    pub rag_top_k: i32,
    pub rag_threshold: f64,
    pub include_citations: bool,
}
```

#### 2.2 Database Migration for RAG Settings

**New File**: `layercake-core/src/database/migrations/m20251112_000022_add_rag_to_chat_sessions.rs`

```sql
ALTER TABLE chat_sessions ADD COLUMN enable_rag BOOLEAN NOT NULL DEFAULT 1;
ALTER TABLE chat_sessions ADD COLUMN rag_top_k INTEGER NOT NULL DEFAULT 5;
ALTER TABLE chat_sessions ADD COLUMN rag_threshold REAL NOT NULL DEFAULT 0.7;
ALTER TABLE chat_sessions ADD COLUMN include_citations BOOLEAN NOT NULL DEFAULT 1;
```

#### 2.3 Update Chat Session Entity

**File**: `layercake-core/src/database/entities/chat_sessions.rs`

```rust
pub struct Model {
    // ... existing fields ...

    pub enable_rag: bool,
    pub rag_top_k: i32,
    pub rag_threshold: f64,
    pub include_citations: bool,
}
```

### Phase 3: Frontend Integration

#### 3.1 Chat Settings UI

**File**: `frontend/src/components/chat/ChatSettings.tsx`

Add RAG configuration section:

```tsx
<Card>
  <CardHeader>
    <CardTitle>Knowledge Base</CardTitle>
  </CardHeader>
  <CardContent className="space-y-4">
    <div className="flex items-center justify-between">
      <Label htmlFor="enable-rag">Enable Knowledge Base</Label>
      <Switch
        id="enable-rag"
        checked={enableRag}
        onCheckedChange={setEnableRag}
      />
    </div>

    {enableRag && (
      <>
        <div className="space-y-2">
          <Label htmlFor="rag-top-k">
            Context Chunks (1-10)
          </Label>
          <Slider
            id="rag-top-k"
            min={1}
            max={10}
            value={[ragTopK]}
            onValueChange={([value]) => setRagTopK(value)}
          />
          <p className="text-xs text-muted-foreground">
            {ragTopK} chunk{ragTopK !== 1 ? 's' : ''} will be retrieved
          </p>
        </div>

        <div className="space-y-2">
          <Label htmlFor="rag-threshold">
            Relevance Threshold (0-100%)
          </Label>
          <Slider
            id="rag-threshold"
            min={0}
            max={100}
            value={[ragThreshold * 100]}
            onValueChange={([value]) => setRagThreshold(value / 100)}
          />
          <p className="text-xs text-muted-foreground">
            Only include chunks with â‰¥{(ragThreshold * 100).toFixed(0)}% relevance
          </p>
        </div>

        <div className="flex items-center justify-between">
          <Label htmlFor="include-citations">Show Sources</Label>
          <Switch
            id="include-citations"
            checked={includeCitations}
            onCheckedChange={setIncludeCitations}
          />
        </div>
      </>
    )}
  </CardContent>
</Card>
```

#### 3.2 Citation Display in Messages

**File**: `frontend/src/components/chat/ChatMessage.tsx`

```tsx
const ChatMessage: React.FC<{ message: Message }> = ({ message }) => {
  // Parse citations from message
  const [content, citations] = parseCitations(message.content)

  return (
    <div className="message">
      <ReactMarkdown>{content}</ReactMarkdown>

      {citations.length > 0 && (
        <div className="mt-4 pt-4 border-t">
          <p className="text-sm font-semibold mb-2">Sources:</p>
          <ul className="text-sm space-y-1">
            {citations.map((citation, i) => (
              <li key={i} className="flex items-center gap-2">
                <IconFileText className="h-3 w-3" />
                <span className="text-muted-foreground">{citation}</span>
              </li>
            ))}
          </ul>
        </div>
      )}
    </div>
  )
}

function parseCitations(content: string): [string, string[]] {
  const parts = content.split('\n---\n**Sources:**\n')
  if (parts.length === 1) return [content, []]

  const citations = parts[1]
    .split('\n')
    .filter(line => line.startsWith('- '))
    .map(line => line.substring(2))

  return [parts[0], citations]
}
```

### Phase 4: Advanced Features

#### 4.1 Conversation History RAG

Extend RAG to include previous conversation turns:

```rust
pub struct ConversationRag {
    /// Recent messages from this session
    conversation_window: Vec<ChatMessage>,

    /// Maximum conversation messages to include
    max_history: usize,
}

impl ConversationRag {
    pub fn build_context(&self, rag_context: RagContext) -> String {
        let mut full_context = String::new();

        // Add conversation history
        if !self.conversation_window.is_empty() {
            full_context.push_str("# Recent Conversation\n\n");
            for msg in &self.conversation_window {
                full_context.push_str(&format!("{}: {}\n\n", msg.role, msg.content));
            }
            full_context.push_str("---\n\n");
        }

        // Add document context
        full_context.push_str(&rag_context.to_context_string());

        full_context
    }
}
```

**Rationale**: Combines conversation context with document context for better coherence.

#### 4.2 Hybrid Search (Future)

Combine vector similarity with keyword matching:

```rust
pub struct HybridSearchParams {
    pub vector_weight: f32,      // 0.0-1.0
    pub keyword_weight: f32,     // 0.0-1.0
    pub min_keyword_matches: usize,
}

pub async fn hybrid_search(
    &self,
    project_id: i32,
    query: &str,
    embedding: &[f32],
    params: HybridSearchParams,
) -> Result<Vec<VectorSearchResult>> {
    // 1. Vector similarity search
    let vector_results = self.vector_store
        .similarity_search(project_id, embedding, top_k)
        .await?;

    // 2. Keyword search (SQLite FTS5)
    let keyword_results = self.keyword_search(project_id, query).await?;

    // 3. Merge and re-rank
    self.merge_results(vector_results, keyword_results, params)
}
```

**Rationale**: Improves recall for domain-specific terms and proper nouns.

#### 4.3 Re-ranking (Future)

Use a cross-encoder model to re-rank retrieved chunks:

```rust
pub struct RerankerService {
    model: CrossEncoderModel,
}

impl RerankerService {
    pub async fn rerank(
        &self,
        query: &str,
        candidates: Vec<VectorSearchResult>,
        top_k: usize,
    ) -> Result<Vec<VectorSearchResult>> {
        let mut scored = Vec::new();

        for candidate in candidates {
            let score = self.model
                .score_pair(query, &candidate.content)
                .await?;

            scored.push((score, candidate));
        }

        scored.sort_by(|a, b| b.0.partial_cmp(&a.0).unwrap());

        Ok(scored.into_iter()
            .take(top_k)
            .map(|(_, result)| result)
            .collect())
    }
}
```

**Rationale**: Improves relevance by using a more sophisticated scoring model.

## Testing Strategy

### Unit Tests âœ… COMPLETED

1. **RagContextBuilder** (6 tests in rag.rs)
   - Token budget enforcement
   - Threshold filtering
   - Citation generation
   - Empty context handling
   - Boundary conditions

2. **Embedding Integration** (covered in integration tests)
   - Query embedding matches document embedding model
   - Error handling for missing embeddings

### Integration Tests âœ… COMPLETED

**File**: `layercake-core/tests/rag_integration_test.rs` (7 tests)

1. **Threshold Filtering**
   - `test_rag_context_threshold_filtering`: Verifies score >= threshold included
   - `test_rag_threshold_boundary`: Tests exact 0.7 threshold boundary
   - `test_rag_all_below_threshold`: All results filtered out correctly

2. **Token Budget Management**
   - `test_rag_token_budget`: Respects 2000 token limit
   - Prevents context overflow

3. **Result Handling**
   - `test_rag_preserves_order`: Maintains highest-to-lowest score order
   - `test_rag_empty_results`: Graceful handling of no results
   - `test_rag_missing_filename`: Defaults to "Unknown source"

### Manual Testing TODO

1. **End-to-End RAG Flow**
   - Upload document via GraphQL
   - Enable embeddings and index
   - Create chat session with RAG enabled
   - Ask question about document
   - Verify response includes context
   - Verify citations present (when frontend ready)

2. **Edge Cases**
   - No indexed documents â†’ graceful fallback âœ… (tested)
   - Empty search results â†’ general knowledge response âœ… (tested)
   - Large documents â†’ token budget respected âœ… (tested)

### Manual Testing Checklist

- [ ] RAG on/off toggle works
- [ ] Similarity threshold affects results
- [ ] Top-k parameter controls chunk count
- [ ] Citations link to correct files
- [ ] Performance acceptable (<2s response time)
- [ ] Works with both OpenAI and Ollama embeddings

## Performance Considerations

### Database Optimization

1. **Index on project_id + file_id**
   ```sql
   CREATE INDEX idx_kb_documents_project_file
   ON kb_documents(project_id, file_id);
   ```

2. **Pre-compute embeddings**
   - All embeddings computed during ingestion
   - No runtime embedding generation for documents

### Token Budget Management

1. **Default limits**
   - Max context: 4000 tokens (~16KB text)
   - Max chunks: 5
   - Reserve tokens for conversation history

2. **Dynamic adjustment**
   - Monitor response quality
   - Adjust based on model context window
   - GPT-4: 8K context â†’ allow more chunks
   - GPT-3.5: 4K context â†’ fewer chunks

### Caching Strategy

1. **Query embedding cache**
   - Cache recent query embeddings (LRU, 100 entries)
   - Key: hash(query_text + model)
   - TTL: 5 minutes

2. **Search results cache**
   - Cache search results per query
   - Invalidate on document updates

## Migration Path

### Phase 1: Backend âœ… COMPLETED
- [x] Create `rag.rs` module (completed 2025-11-10)
- [x] Extend `ChatSession` with RAG config (completed 2025-11-10)
- [x] Integrate RAG into conversation flow (completed 2025-11-10)
- [x] Write unit tests for RagContextBuilder (completed 2025-11-10)
- [x] Write integration tests for RAG (completed 2025-11-10)
- [x] Add `embed_text()` to EmbeddingService (completed 2025-11-10)
- [x] Update VectorSearchResult with file metadata (completed 2025-11-10)
- [x] Add database migration (completed 2025-11-10)
- [x] Update chat session entity (completed 2025-11-10)
- [x] Persist RAG settings in database (completed 2025-11-10)

### Phase 2: API âœ… COMPLETED
- [x] Update GraphQL types (completed 2025-11-10)
- [x] Add RAG fields to queries (completed 2025-11-10)
- [ ] Add RAG fields to mutations (deferred - not critical for MVP)
- [ ] Test with GraphQL Playground

### Phase 3: Frontend ðŸŸ¡ PARTIALLY COMPLETE
- [x] Add RAG status indicators to chat UI (completed 2025-11-10)
- [x] Display RAG settings in chat logs table (completed 2025-11-10)
- [ ] Add RAG configuration controls (deferred - using database defaults)
- [ ] Implement citation display in messages (deferred - backend ready)
- [ ] Update session creation dialog with RAG options (deferred)
- [ ] Manual end-to-end testing

### Phase 4: Polish (Week 2)
- [ ] Performance testing
- [ ] Documentation
- [ ] User guide for RAG features

## Implementation Progress

### Completed (2025-11-10)

#### Phase 1: Core RAG Infrastructure âœ…
- **`rag.rs` module**: Created with `RagContext`, `RagChunk`, and `RagContextBuilder`
  - Threshold filtering (0.0-1.0)
  - Token budget management (default: 4000 tokens)
  - Context string formatting for LLM prompts
  - Citation generation for source attribution
  - Comprehensive unit tests

- **ChatSession integration**:
  - Added RAG configuration fields: `rag_enabled`, `rag_top_k`, `rag_threshold`, `include_citations`
  - Integrated `DataAcquisitionService` for embeddings and vector search
  - Implemented `get_rag_context()` method for retrieval
  - Modified `build_conversation_prompt()` to prepend RAG context to system prompt
  - Loads RAG settings from database on session resume
  - Default settings: enabled=true, top_k=5, threshold=0.7, citations=true

- **Data acquisition enhancements**:
  - Added `embed_text()` method to `EmbeddingService` for single-query embeddings
  - Updated `VectorSearchResult` struct to include `file_id` and `filename`
  - Modified `similarity_search()` to join with files table for metadata
  - Added `Related` trait implementation for kb_documents->files
  - Public accessor for embeddings service

#### Phase 2: Database & API âœ…
- **Database migration m20251112_000022**:
  - Added columns: `enable_rag`, `rag_top_k`, `rag_threshold`, `include_citations`
  - Default values match code defaults (true, 5, 0.7, true)
  - SQLite-compatible rollback support

- **Entity updates**:
  - Updated `chat_sessions::Model` with RAG fields
  - RAG configuration persisted and restored across sessions

- **GraphQL API**:
  - Added RAG fields to `ChatSession` type
  - Fields exposed in all session queries (list, get)
  - Frontend can now read RAG configuration

#### Phase 3: Frontend ðŸŸ¡ (Partial)
- **UI Indicators** (completed 2025-11-10):
  - Added RAG status badge to `ProjectChatPage` (ðŸ“š RAG)
  - Added RAG column to `ChatLogsPage` sessions table
  - Badge shows "On" with settings tooltip or "Off"
  - Tooltip displays: top-k, threshold %, citations status

- **GraphQL Integration**:
  - Updated TypeScript `ChatSession` interface with RAG fields
  - Modified `GET_CHAT_SESSIONS` query to fetch RAG data
  - Frontend successfully builds and displays RAG status

**Deferred to Future**:
- RAG configuration controls (currently uses database defaults)
- Citation display in chat messages (backend ready, UI pending)
- Session creation dialog with RAG options

### Production Ready âœ…
- Core RAG retrieval and context injection
- Database persistence of RAG settings
- GraphQL API with RAG fields
- Frontend status indicators
- All tests passing (13/13)

### Deferred Features (Non-Critical)
- GraphQL mutation to update RAG settings per session
- Frontend controls for RAG configuration (currently uses DB defaults)
- Citation display in chat message UI (backend generates citations)
- Session creation dialog with RAG options

### Current Behavior
- RAG enabled by default (database default: true)
- Default settings: top_k=5, threshold=0.7, citations=true
- Settings visible in UI but not yet editable
- Citations generated in backend but not parsed/displayed in frontend

## Configuration

### System Settings

Add to `system_settings` table:

```yaml
rag:
  default_top_k: 5
  default_threshold: 0.7
  max_context_tokens: 4000
  enable_citations: true
  enable_hybrid_search: false  # Future
```

### Per-Session Override

Users can override defaults when creating chat sessions.

## Security Considerations

1. **Access Control**
   - Only retrieve documents from user's project
   - Filter by `project_id` in all queries
   - Verify user has access to project

2. **Prompt Injection Protection**
   - Sanitize retrieved content
   - Clearly separate context from instructions
   - Use structured prompts

3. **Data Privacy**
   - Documents never sent to external services (already embedded)
   - Only embeddings and retrieved chunks used

## Monitoring & Observability

### Metrics to Track

1. **RAG Performance**
   - Average chunks retrieved per query
   - Average similarity score
   - Context utilization (% of token budget used)
   - Cache hit rate

2. **Quality Metrics**
   - User feedback on responses
   - Citation click-through rate
   - Follow-up question frequency

### Logging

```rust
tracing::info!(
    project_id = session.project_id,
    chunks_retrieved = rag_context.chunks.len(),
    avg_score = avg_score,
    total_tokens = rag_context.total_tokens,
    "RAG context built"
);
```

## Open Questions

1. **Should we support multiple embedding models per project?**
   - Current: Single embedding model per knowledge base
   - Future: Allow users to choose model per session?

2. **How to handle document updates?**
   - Current: Rebuild index manually
   - Future: Incremental updates with versioning?

3. **Multi-lingual support?**
   - Current: Embeddings work for any language
   - Future: Language-specific chunking/tokenization?

## References

- [Rig documentation](https://github.com/0xPlaygrounds/rig)
- [RAG best practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- [OpenAI embeddings guide](https://platform.openai.com/docs/guides/embeddings)
