TOOLCHAIN_VERSION=stable

# =============================================================================
# LLM Chat Configuration
# =============================================================================

# Default chat provider (openai, claude, gemini, ollama)
# Default: ollama
LAYERCAKE_CHAT_PROVIDER=ollama

# Chat request timeout in seconds
# Default: 90
LAYERCAKE_CHAT_TIMEOUT_SECS=90

# System prompt for all chat sessions (optional)
# Default: none
LAYERCAKE_CHAT_SYSTEM_PROMPT=

# Preferred embedding provider for knowledge base indexing (openai, ollama)
# Default: openai
LAYERCAKE_EMBEDDING_PROVIDER=openai

# MCP server URL for tool integration
# Default: http://localhost:3000/mcp
LAYERCAKE_MCP_SERVER_URL=http://localhost:3000/mcp

# =============================================================================
# OpenAI Configuration
# =============================================================================

# OpenAI model to use
# Default: gpt-4o-mini
LAYERCAKE_OPENAI_MODEL=gpt-4o-mini

# OpenAI API key (required for OpenAI provider)
# Default: none
OPENAI_API_KEY=

# OpenAI base URL (optional, for custom endpoints)
# Default: none (uses OpenAI's official API)
OPENAI_BASE_URL=

# OpenAI embedding model
# Default: text-embedding-3-large
LAYERCAKE_OPENAI_EMBEDDING_MODEL=text-embedding-3-large

# =============================================================================
# Anthropic Claude Configuration
# =============================================================================

# Claude model to use
# Default: claude-3-5-sonnet-20241022
LAYERCAKE_CLAUDE_MODEL=claude-3-5-sonnet-20241022

# Anthropic API key (required for Claude provider)
# Default: none
ANTHROPIC_API_KEY=

# =============================================================================
# Google Gemini Configuration
# =============================================================================

# Gemini model to use
# Default: gemini-2.0-flash-exp
LAYERCAKE_GEMINI_MODEL=gemini-2.0-flash-exp

# Google API key (required for Gemini provider)
# Default: none
GOOGLE_API_KEY=

# =============================================================================
# Ollama Configuration
# =============================================================================

# Ollama model to use
# Default: llama3.2
LAYERCAKE_OLLAMA_MODEL=llama3.2

# Ollama embedding model
# Default: nomic-embed-text-v1.5
LAYERCAKE_OLLAMA_EMBEDDING_MODEL=nomic-embed-text-v1.5

# Ollama base URL
# Default: http://127.0.0.1:11434
OLLAMA_BASE_URL=http://127.0.0.1:11434

# Ollama API key (optional, usually not needed for local Ollama)
# Default: none
OLLAMA_API_KEY=
